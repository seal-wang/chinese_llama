# chinese_llama
Created by seadog-www
## 📝介绍
现在，主流大模型的参数量都是Billion级以上的，如此庞大的参数量，限制了个人对大模型全流程的探索，因此，本项目将构建一个可在消费级显卡上进行预训练、微调和推理的小参数量的中文Llama2模型。虽然模型的参数量较小，要求其具有较好的表现不太现实，但是，还是希望模型能在某些方面具有一定的表达能力，以验证整个工作可行性和价值。

包含：数据清洗、token模型裁剪、预训练、SFT指令微调完整流程。

## 🤖PT
1. **预训练语料（Corpus for pre-training ）**
   - **通用百科语料**：[中文维基百科](https://dumps.wikimedia.org/zhwiki/)、[百度百科](https://huggingface.co/datasets/xuqinyang/BaiduBaike-5.63M)
   - **医学领域语料**：[中文医疗数据集](https://huggingface.co/datasets/shibing624/medical)
2. **语料处理**

   中文维基百科语料比较原始，包含了很多于训练无益的符号、标签以及引用等内容，因此需要进行清洗，gensim库的wikicorpus模块是可以用来清洗wiki语料的，wikicorpus主要是通过正则表达式来对文档进行处理，由于文档中存在嵌套的结构，因此实际处理后还是会存在一些问题，如人物的生卒年月被删除、引用和文件等信息残留。因此，需要结合wikicorpus进一步完善数据的清洗。
   执行脚本[wiki_clean.py](https://github.com/seadog-www/chinese_llama/blob/main/utils/wiki_clean.py)清洗wiki语料
   ```bash
   ./wiki_clean.py -i zhwiki_file [zhwiki_file ...] -o out_path
   ```
3. **分词模型裁剪**

   本项目不再单独生成一个分词模型，而是直接用chatGLM的分词模型。同时考虑到以下两点：
   - 本项目模型参数较小，如果vocab_size太大，势必会造成embedding部分占据参数量的很大比例，造成Attention和FFN部分参数不足，从而影响模型表现；
   - 预训练的语料主要是中文文本，而chatGLM的分词模型中包含了除中文字符以外的很多其他字符，如英语、阿拉伯语、希伯来语等；
   需要对chatGLM的分词模型进行裁剪，保留全部中文、一小部分英文和其他符号，裁剪后的分词模型包含了35023个token。
   执行脚本[strip_token_model.py](https://github.com/seadog-www/chinese_llama/blob/main/utils/strip_token_model.py)裁剪分词模型
   ```bash
   # retain_en.txt包含需要保留或添加的英文tokens
   ./strip_token_model.py -i old_token.model -o new_token.model -a retain_en.txt
   ```
4. **确定训练参数**
	本项目是在3060显卡（显存6G）上运行的，模型参数约92M，开启了activation_checkpointing和fp16混合精度训练以节省显存，训练时batch_size大小为4，模型参数如下：

   | 项目                         | 值                                                    |
   |---------------------------------|----------------------------------------------------------------|
   | hidden_size               | 768                                        |
   | intermediate_size               | 1668                                        |
   | num_hidden_layers               | 6                                        |
   | num_attention_heads               | 8                                        |
   | max_position_embeddings               | 512                                        |
   可以根据实际的硬件情况调整相关参数。
5. **开始预训练**

	执行如下脚本开启预训练,并记录日志。本项目将训练两个模型
   - 通用模型（common base model），其训练语料仅包含通用百科语料（中文维基百科+百度百科）；
   - 是医学模型（medical base model），其训练语料包含通用百科语料和医学领域语料（中文维基百科+百度百科+中文医疗数据集）
	```bash
	./run_clm.sh | tee train.log
	```
   
## 💡SFT指令微调
LLM微调的目的是让模型能够按我们预想的方式来整合其蕴含知识，并按人类的习惯进行表达。
1. **微调方法**

	目前大模型通常采用PEFT来微调模型，兼顾了模型的效果和微调的效率，由于本项目模型较小，仅采用这种高效微调的方式效果可能不太理想，因此，对预训练的基座模型，分别采用full fine-tuning和lora两种方式来微调，并对比这两种微调后的模型的表现。
2. **SFT微调数据**
   
   - 通用模型主要包含这种百科知识，微调后，希望它可以进行日常的QA对话，因此，微调通用模型（common base model）时，选择常用的Belle数据集中的[train_1M_CN](https://huggingface.co/datasets/BelleGroup/train_1M_CN)，它包含100万条涵盖多个领域的中文指令数据
   - 医学模型中包含医学领域中的一些知识，微调后，希望它可以在健康、疾病等方面进行QA对话，因此，微调医学模型（medical base model）时，选择[中文医疗数据集](https://huggingface.co/datasets/shibing624/medical)中的中文finetune数据，它包含多个科室的上百万条问诊对话数据
3. **开始微调**

	全参微调full fine-tuning
	```bash
	./run_sft_with_full.sh | tee train.log
	```
	lora微调lora fine-tuning
	```bash
	./run_sft_with_lora.sh | tee train.log
	```

3. **合并lora权重**

	执行脚本[merge_lora.py](https://github.com/seadog-www/chinese_llama/blob/main/utils/merge_lora.py)，合并lora权重到base模型
	```bash
	python merge_lora.py -b base_path -l lora_path -o output_path
	```

## 🥇模型权重及测试
1. **预训练模型**
   
   | 模型名称                                                        | 预训练语料                                                      | 下载地址                                                            |
   |-------------------------------------------------------------|------------------------------------------------------------|-----------------------------------------------------------------|
   | common base model                                       | 中文维基百科<br/>+百度百科<br/>     | [模型下载](https://drive.google.com/drive/folders/1f2oSSIV_9VkS3qOfgxqm1xrLWvQRK3Az?usp=sharing) |
   | medaical base model                                       | 中文维基百科<br/>+百度百科<br/>+医学领域语料<br/> | [模型下载](https://drive.google.com/drive/folders/1QD3LD-MgJ0jpj1f2gSRBERl9H09fd7hy?usp=sharing) |
   
2. **微调模型**
   
   | 模型名称                               | SFT语料                                                                                     | 🤗微调方式                                          | 下载地址                                                            |
   |------------------------------------|-------------------------------------------------------------------------------------------|-------------------------------------------------|-----------------------------------------------------------------|
   | common model with full  | BelleGroup/train_1M_CN                                                                            |  full fine-tuning   | [模型下载](https://drive.google.com/drive/folders/1CZcCx-M2iRBdgixg7QUUTR52eF6ravs4?usp=sharing) |
   | medaical model with full | shibing624/medical | full fine-tuning    | [模型下载](https://drive.google.com/drive/folders/1LgzKYTxOnidseGQZIzfr3hlpipPQd2UM?usp=sharing) |
   | common model with lora   | BelleGroup/train_1M_CN                                                                           | lora fine-tuning    | [模型下载](https://drive.google.com/drive/folders/1xhlw_f7ypKQ7aa5zLIunT36HHOAH5byi?usp=sharing) |
   | medaical model with lora  | shibing624/medical | lora fine-tuning    | 效果不佳，在进一步探索中！！！                                                      |

3. **模型测试**

	1. 下载相应的模型文件
	2. cd到模型目录中
		```bash
		cd target_dir
		```
	3. 执行生成脚本generate.py
		```
		python generate.py
		```
	相对于生成的多样性要求，本项目的小模型更侧重生成的准确性和稳定性，因此生成时未使能采样策略，full fine-tuning的模型采用的是贪婪搜索，lora fine-tuning的模型因为效果较差所以采用了束搜索的生成策略（beam_num=3），可以修改相应的generate.py脚本，调整生成策略参数，尝试更多效果。

	**common model with full**
   ```
	Q: 请介绍一下北京
	A: 北京是中国的首都，是中国最大的城市之一。它有着悠久的历史和丰富的文化遗产，包括故宫、长城等著名景点。
	北京最著名的地标之一就是北京的八达岭长城。这座历史悠久的建筑群不仅展示了古代的军事防御能力，还代表了中国古代文化的辉煌与丰富。此外还有许多美丽的公园和花园供人们休闲娱乐活动。
	在历史上，北京曾经是一个重要的商业中心，也是中国最重要的贸易和文化中心之一。在这里，你可以品尝到各种美食，如烤鸭、炸酱面等等。同时，这里也是一个充满文化气息的地方，有许多博物馆和历史遗迹可以了解这个地区的文化遗产。

	Q: 列举5种常见的水果
	A:
	苹果、香蕉、草莓、蓝莓和覆盆子。

	Q: 世界上最大的动物是？
	A: 蓝鲸是世界上最大的动物。

	Q: 请推荐一份健康的早餐
	A: 一个健康的早餐可以包括以下几个方面：
	1. 燕麦粥配水果和坚果。
	2. 全谷物面包夹鸡蛋、全麦吐司片和蔬菜。
	3. 水煮蛋配低脂奶酪，如菠菜或杏仁奶。
	4. 烤全麦土司配牛油果酱和新鲜的水果。

	Q: 高血压患者在饮食上有什么需要注意的点？
	A:
	高血压患者需要注意以下几点：
	1. 避免过度摄入盐和糖，以降低血压。
	2. 在进食时保持规律，不要暴饮暴食或吃太多不健康食物。
	3. 如果出现高血压症状，应该咨询医生并遵循医生的建议进行治疗。

	Q: 你知道西安有什么特色美食吗？
	A: 西安有独特的烤肉、糖醋排骨和葱爆羊肉等传统小吃，还有各种不同的口味。
   ```
  
	**medaical model with full**
	
   ```
    Q: 吃芒果后，嘴角起泡，这是什么原因？
    A: 你好，这种情况考虑是局部感染引起的症状。建议你注意保持局部的干燥卫生，可以外用百多邦等药物进行治疗看看效果的，如果不能改善的话，必要时到医院检查一下分泌物常规明确病因对症处理
    # 实际应该是对芒果过敏，当然不排除感染也会有相同症状

    Q: 运动后脚后跟疼是怎么回事？
    A: 考虑和受凉，或运动不协调有关。建议注意休息，避免剧烈活动，可以热敷一下缓解疼痛症状，口服消炎药物头孢克肟分散片调理看看，必要时可以热敷一下缓解不适的症状，慢慢会改善的
    # 确实是运动不协调，脚跟先着地容易有这种问题，减少运动量后会自愈

    Q: 有时会突然很饿,出虚汗,全身没力这是怎么一回事？
    A: 可能是贫血，建议你去医院做个血常规检查。 ，除了正规治疗血液内科疾病外，患者需要多咨询专家建议，和医生保持沟通，患者还需要重视饮食方面，例如日常清淡饮食，多外出运动。与此同时患者还要注意选择一家专业医院诊治，这样才能得到良好的效果。
    # 也可能是低血糖

    Q: 高血压有什么饮食禁忌需要注意？
    A: 血压高可以引起动脉硬化，所以需要低盐、低脂饮食。平时注意多吃蔬菜水果及粗粮，保持大便通畅。

    Q: 拉肚子可以吃什么药进行治疗？
    A: 建议口服补液盐或高渗氯化钠溶液加压包扎。注意多喝水，忌辛辣刺激性饮食。必要时输液治疗。
    # 还需要进行消炎和止泻治疗

    Q: 脖子上长了丘疹和小疙瘩是怎么回事？
    A: 考虑是毛囊炎，建议注意多喝水,可以涂点红霉素软膏或百维素进行治疗。也可服用消炎药头孢菌如清开灵胶囊等药物对症治疗的
    # 也可能是痤疮

    Q: 每当冬季时，鼻炎就会加重，该如何缓解？
    A: 你好，根据你的情况考虑你可能是由于过敏性体质引起的。建议你可以服用抗敏药物如西替利嗪、氯雷他定等药物治疗看看，平时注意做好保暖护理即可
    # 西替利嗪和氯雷他定确实之用来缓解和治疗鼻炎的
   ```

	**common model with lora**
	
   ```
	Q: 请介绍一下北京
	A: 北京是中国最大的城市之一，位于中国北京市的中心位置，拥有悠久的历史和深厚的文化底蕴。
	北京是中国的政治、经济和文化中心，也是世界闻名的旅游胜地。北京的旅游景点包括故宫、天坛、颐和园、天坛、颐和园等。此外，北京还有许多著名景点，如王府井、东城区、西城区等。
	北京还以其独特的文化和历史遗迹而闻名于世。北京有许多著名的旅游景点，如皇家园林、清真寺、大观园等等。北京还以其独特的建筑和景观而闻名于世。
	总之，北京是一个充满活力和活力的城市，有着丰富的文化和历史遗迹。北京的文化积淀深厚，吸引了众多游客前来参观游览。无论您是想了解北京的历史和文化，还是想了解北京的历史和文化，北京都能为您提供有益的体验。

	Q: 列举5种常见的水果
	A: 1. 苹果
	2. 香蕉
	3. 菠萝
	4. 芒果
	5. 葡萄柚

	Q: 世界上最大的动物是？
	A: 野生动植物物种的数量和种类繁多，其中最引人注目的动物是豹猫、斑马和企鹅。这些动物的栖息地非常广泛，包括海豹、豹子、熊和北极熊等。
	为了保护野生动物，人们需要采取各种措施来保护它们。首先，要防止猎物被猎杀；其次，应避免在野外捕食；最后，应该尽可能减少对环境的影响，如焚烧垃圾、砍伐树木等。只有这样，我们才能确保它们的安全和健康。
	总之，保护野生动物是我们每个人的责任。让我们共同努力，共同保护我们的地球，让我们的世界更加美好！

	Q: 请推荐一份健康的早餐
	A: 燕麦粥、水果沙拉、全麦吐司等。
	建议选择低脂肪、低糖分的食物，如燕麦、鸡蛋、豆类等。
	另外，建议选择富含纤维素和维生素C的食物，如燕麦、鸡蛋、豆类等。
	最后，建议选择含有蛋白质和碳水化合物的食物，如燕麦、鸡蛋、豆类等。

	Q: 高血压患者在饮食上有什么需要注意的点？
	A: 1. 少吃高热量、高脂肪、高糖分的食物，如水果、蔬菜、全谷类食品等。
	2. 控制摄入量，避免过量食用高热量、高脂肪、高糖分的食物。
	3. 保持良好的睡眠质量，保证充足的睡眠时间。
	4. 定期进行身体检查，发现异常情况及时就医。
	5. 减少饮酒和吸烟对健康的影响，戒烟限酒。
	6. 定期进行体检，确保身体健康。
	7. 坚持锻炼，增强心肺功能和骨骼健康。
	8. 注意饮食卫生，避免暴饮暴食。

	Q: 你知道西安有什么特色美食吗？
	A:
	西安是一个充满活力和活力的城市，有着悠久的历史和文化积淀。它以其独特的文化、美食、美食而闻名于世，被誉为“中国最美丽的旅游胜地之一”。
	西安有许多著名的旅游景点，如长安街、清真寺、大雁塔等。其中最有名的景点包括：长安城、天宁寺、三清山、西岳华山、东岳泰山、南岳嵩山等。此外，西安还有许多著名景点，如钟楼、鼓楼、关帝庙等。
	总之，西安是一个充满活力和活力的城市，拥有丰富的历史和文化底蕴。它以其独特的文化、美食、美食而闻名于世，被誉为“中国最美丽的旅游胜地之一”。
   ```

综合试用下来，有如下几点感受：
   1. 模型在表达上没有明显的语义不通或语法错误问题，能进行一些简单QA交流；
   2. 对于一些问题能从多个方面回答出问题的要点，比如关于某个症状的原因；
   3. 回答稳定性欠佳，同一个问题稍作变换，回答可能会截然不同；
   4. 有些回答虽然没有错误，但是内容偏空泛，实质性不强；
   5. 小参数的模型全参微调与部分参数微调的效果差距比较明显，lora微调的common model在生成时，即使开启束搜索（beam_num=3），效果较贪婪搜索的full fine-tuning模型还是偏差。
